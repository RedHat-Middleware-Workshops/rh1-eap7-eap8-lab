== Access Your Development Environment

You will be using Red Hat OpenShift Dev Spaces. **Changes to files are auto-saved every few seconds**, so you don't need to explicitly save changes.

. To get started, {DS_URL}[access the Red Hat OpenShift Dev Spaces^] you will be presented with an option to login with your OpenShift credentials
+
image::openshift-login.png[openshift-login]

. Click on "Login with OpenShift"
+
image::openshift-username.png[openshift-username]

. Log in using the username and password you've been assigned:

* *Username*: `{USER_ID}`
* *Password*: `openshift`
+
You will be prompted to Authorize access to the devspaces client to access your account.  Click on "Allow Selected Permissions"

. Once you log in, you’ll be placed on your personal dashboard. 
+
image::rhsi-devspaces-dashboard.png[rhsi-devspaces-dashboard]

. Click on `Open`
+
image::rhsi-devspace-open.png[rhsi-devspace-open]
+
After a minute or two, you’ll be placed in the workspace.  

. When the workspace first loads you will be prompted to with a message "Do you trust the authors of the files in this workspace?" 
+
image::rhsi-devspaces-trust.png[rhsi-devspaces-trust]

. Click on "Yes, I trust these authors"
+
image::rhsi-devspaces-start.png[rhsi-devspaces-start]

Users of Eclipse, IntelliJ IDEA or Visual Studio Code will see a familiar layout: a project/file browser on the left, a code editor on the right, and a terminal at the bottom. You'll use all of these during the course of this workshop, so keep this browser tab open throughout. **If things get weird, you can simply reload the browser tab to refresh the view.**

=== Logging into the Openshift cluster using devspaces terminal

. Access the terminal in dev spaces by clicking on "Menu" -> "Terminal" -> "New Terminal"
+
image::open-terminal.png[open-terminal]

. *Switch to the project called "{USER_ID}-project"*
+
[source,sh,role="copypaste",subs=attributes+]
----
oc project {USER_ID}-project
----

. You should see an output similar to the one below
+
[source,sh,role="copypaste",subs=attributes+]
----
Using project "user1-project" on server
----

. Since we'll be using multiple terminal sessions in this exercise (one for OpenShift and other for the VM). Let's rename this terminal tab to avoid confusion. Click on the `bash` icon as show below and select `Rename`option
+
image::rhsi-rename-ocp.png[rhsi-rename-ocp]


. Rename it to `OCP` and hit enter
+
image::rhsi-enter-ocp.png[rhsi-enter-ocp]

. Your terminal should look something like this
+
image::rhsi-only-ocp-terminal.png[rhsi-only-ocp-terminal]

=== Logging into the VM using devspaces terminal
. Create a new terminal window to log into the VM by clicking the split terminal icon
+
image::rhsi-split-terminal.png[rhsi-split-terminal]

. Rename newly created terminal window (the one to the right). Right click on the `bash` icon as show below and select `Rename`option
+
image::rhsi-rename-vm.png[rhsi-rename-vm]

. Rename it to `VM` and hit enter
+
image::rhsi-enter-vm.png[rhsi-enter-vm]

. Your terminal should look something like this
+
image::rhsi-terminal.png[rhsi-terminal]

. Login to the VM using this newly created terminal tab+
+
[source,sh,role="copypaste",subs=attributes+]
----
ssh -o ServerAliveInterval=60 {bastion_ssh_user_name}@{bastion_public_hostname}
----

. Enter the following password to login
+
*Password:* `{bastion_ssh_password}`

. If the login has been successful you should be logged in as `lab-user` on the VM
+
image::rhsi-vm-terminal.png[rhsi-vm-terminal]


## Deployment overview
A legacy soap API has been deployed on the VM






There are multiple ways to setup layer 7 connectivity using Red Hat Service interconnect. You use either use the CLI or the OpenShift Operator. In this section we'll be using the operator to setup connectivity on the OpenShift end and the CLI to setup the connectivity on the VM side.

## Initialize Red Hat Service Interconnect in the Openshift cluster

As mentioned earlier, we could either use the CLI or operator to initialize Red Hat Service Interconnect. To showcase both the options we'll be using the operator on OpenShift end and CLI on the VM build the connectivity. 

. From the devspaces UI navigate to the terminal on OpenShift where you logged into the OpenShift Cluster
+
image::rhsi-ocp-terminal.png[rhsi-ocp-terminal]

. Login to the right project
+
[source,sh,role="copypaste",subs=attributes+]
----
oc project {USER_ID}-project
----

. Create a config file with the name `mysite.yaml`
+
[source,sh,role="copypaste",subs=attributes+]
----
vi mysite.yaml
----

. Paste the below contents in `mysite.yaml` and save it using the buttons esc followed by :wq and hit enter to save the file
+
[source,yaml,role="copypaste"]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: skupper-site
data:
  name: ocp
  console: "true"
  console-user: "admin"
  console-password: "openshift"
  flow-collector: "true"
----


. Initialize the Service Interconnect Router by issuing the below command in the OCP terminal. This should install the Service Interconnect resources in the namespace
+
[source,sh,role="copypaste",subs=attributes+]
----
oc apply -f mysite.yaml
----

. Navigate to the Red Hat Service Interconnect console to verify that OCP cluster appears in the topology. Once the connectivity is established we should be able to see the VM in the topology view too
+
[source,sh,role="copypaste",subs=attributes+]
----
https://skupper-{openshift_cluster_user_name}-project.{openshift_cluster_ingress_domain
}/#/topology
----

. Navigate to the Red Hat Service Interconnect console to verify that OpenShift cluster (OCP) appears in the topology. Once the connectivity is established we should be able to see the VM in the topology view too. To get the URL of the console copy the output of the below command and paste in the browser
+
[source,sh,role="copypaste",subs=attributes+]
----
oc get routes skupper -o jsonpath='{.spec.host}'
----
+
Ignore any warnings and proceed by pressing the advanced link on your browser

. Login with the following credentials:
+
* *Username*: `admin``
* *Password*: `openshift`

. You should now be able to see that Openshift cluster (ocp) appears in the topology.
+
image::rhsi-ocp-topology.png[rhsi-ocp-topology]

## Initialize Red Hat Service Interconnect in the RHEL VM
We'll be using the CLI to initialize Red Hat Service Interconnect \on the VM.  

. Go to the terminal on the devspaces where you are logged in to the VM. The Red Hat Service Interconnect cli is already available on the VM.
+
image::rhsi-vm-terminal.png[rhsi-vm-terminal]

. Install the Red Hat Service Interconnect CLI
+
[source,sh,role="copypaste",subs=attributes+]
----
curl https://skupper.io/install.sh | sh
----

. Switch the skupper cli podman site mode as we will be using podman to run our skupper containers
+
[source,sh,role="copypaste",subs=attributes+]
----
export SKUPPER_PLATFORM=podman
----

. Confirm the same the running the below command. The output should say *podman*
+
[source,sh,role="copypaste",subs=attributes+]
----
skupper switch
----

. Initialize the Service Interconnect Router by issuing the below command in the VM terminal.
+
[source,sh,role="copypaste",subs=attributes+]
----
skupper init --ingress none
----




. Output
+
[source,sh,role="copypaste",subs=attributes+]
----
Skupper is now installed for user 'lab-user'.  Use 'skupper status' to get more information.
----

. To see the status of the skupper network
+
[source,sh,role="copypaste",subs=attributes+]
----
skupper status
----

. Output
+
[source,sh,role="copypaste",subs=attributes+]
----
Skupper is enabled for "lab-user" with site name "bastion.p9dj5.internal-lab-user-d196f". It is not connected to any other sites. It has no exposed services.
----
+
This confirms that we have not yet established the connection between the sites.


## Create a link between the namespace on OpenShift cluster and the VM
To create a link between the environments, you create a token on one of the environments, and then use the token to create the link on the other. This token will be used to setup a mTLS (mutual TLS) secured link between the two environments.

. Make sure you are on the OCP terminal
+
image::rhsi-ocp-terminal.png[rhsi-ocp-terminal]

. Create a YAML file named `token-request.yaml`` to request a token
+
[source,sh,role="copypaste",subs=attributes+]
----
vi token-request.yaml
----

. Paste the below contents in `token-request.yaml` and save it using the buttons esc followed by :wq and hit enter to save the file
+
[source,yaml,role="copypaste"]
----
apiVersion: v1
kind: Secret
metadata:
  labels:
    skupper.io/type: connection-token-request
  name: secret-name
----

. Apply the YAML to the namespace to create a secret.
+
[source,sh,role="copypaste",subs=attributes+]
----
oc apply -f token-request.yaml
----

. Create the actual token from the secret
+
[source,sh,role="copypaste",subs=attributes+]
----
oc get secret -o yaml secret-name | yq 'del(.metadata.namespace)' > token.yaml
----

. Display the token and save it in a text editor . We'll use this token on VM to establish the connection
+
[source,sh,role="copypaste",subs=attributes+]
----
cat token.yaml
----
+
This is actually an OpenShift secret which contains a certificate. This certificate will be used to setup a mTLS (mutual TLS) secured link between the two environments. The next step is creating the link on the VM with the token.

. Navigate to the the VM terminal that you have connected earlier to using the terminal on your local machine. Make sure you are logged in as *lab-user@bastion*
+
image::rhsi-vm-terminal.png[rhsi-vm-terminal]

. Create a new file on the VM terminal where you will paste the token you just generated on the OCP cluster.
+
[source,sh,role="copypaste",subs=attributes+]
----
vi secret.token
----

. Paste the token you saved earlier, in this file and save it using the buttons esc followed by :wq and hit enter to save the file. 
+
[NOTE]
====
If you haven't saved the token earlier, you can also copy the contents of the `token.yaml` file as indicated below. Select all and copy the token
====
+
image::rhsi-token-file.png[rhsi-token-file]



. Create a link on the VM using the token
+
[source,sh,role="copypaste",subs=attributes+]
----
skupper link create secret.token --name ocp-to-vm
----

. Output
+
[source,sh,role="copypaste",subs=attributes+]
----
Site configured to link to skupper-inter-router-user1-project.apps.cluster-p9dj5.p9dj5.sandbox211.opentlc.com:443 (name=ocp-to-vm)
Check the status of the link using 'skupper link status'.
----

. Navigate back to the web console tab on your browser and refresh the tab. You should now be able to see the VM in the topology
+
image::rhs-ocp-vm-topology.png[rhs-ocp-vm-topology]
+
This confirms that the link has now been established
+
[NOTE]
====
The circles depicting the environments might overlap each other sometimes and hence might not be visible. If that's the case, click on the circle that's visible and drag it to the side ensure proper visibility.
====


. Finally, you need to expose the soap service running on the VM over the link. This will allow the soap service to be accessed from the openshift cluster as if it was a local service. 
+
[source,sh,role="copypaste",subs=attributes+]
----
skupper expose host host.containers.internal --address soap-api --port 8080
----

. When you are connecting Openshift to the VM using Red Hat Service Interconnect, you have create a proxy service that will redirect to the service running on the VM. To achieve this first make sure you are on the OCP terminal
+
image::rhsi-ocp-terminal.png[rhsi-ocp-terminal]

. Create a yaml file with the name `skupper-service.yaml` 
+
[source,sh,role="copypaste",subs=attributes+]
----
vi skupper-service.yaml
----


. Paste the below contents in `skupper-service.yaml` and save it using the buttons esc followed by :wq and hit enter to save the file
+
[source,yaml,role="copypaste"]
----
kind: Service
apiVersion: v1
metadata:
  name: soap-api
  annotations:
    skupper.io/address: soap-api
    skupper.io/port: "8080"
    skupper.io/proxy: "tcp"
    skupper.io/target: "soap-api"
    
spec:
  ports:
    - name: port8080
      protocol: TCP
      port: 8080
      targetPort: 1025
----

. Create the service
+
[source,sh,role="copypaste",subs=attributes+]
----
oc apply -f skupper-service.yaml
----

## Verify the Service Interconnect network
At this point you can verify that the service network is actually working as expected.

. Make sure you are on the OCP terminal
+
image::rhsi-ocp-terminal.png[rhsi-ocp-terminal]

. Make sure you are on the `rh1-lab-eap-camel-quarkus` folder
+
[source,sh,role="copypaste",subs=attributes+]
----
pwd
----

. Output
+
[source,sh,role="copypaste",subs=attributes+]
----
/projects/rh1-lab-eap-camel-quarkus
----

. Navigate to the `soap1` folder
+
[source,sh,role="copypaste",subs=attributes+]
----
cd migration-fuse-camel-quarkus/base/camelq/stubs/soap1
----

. Send a request to the soap service. Observe that the URL that you are using is local OpenShift service and not any external route exposed to the public internet
+
[source,sh,role="copypaste",subs=attributes+]
----
curl -s -d @src/main/resources/request.xml http://soap-api.user1-project.svc.cluster.local:8080/services/s1 | xmllint --format -
----

. If our Service Interconnect network was setup correctly you should be able to see a response as shown below
+
[source,xml,role="copypaste",subs=attributes+]
----
<?xml version="1.0"?>
<soap:Envelope xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/">
  <soap:Body>
    <ns2:SubscriberResponse xmlns:ns2="http://www.example.org/s1/">
      <Name>Some</Name>
      <Surname>One</Surname>
      <Address>
        <Number>1</Number>
        <Street>Some Street</Street>
        <City>Somewhere</City>
        <PostCode>SOME C0D3</PostCode>
        <Country>UK</Country>
      </Address>
    </ns2:SubscriberResponse>
  </soap:Body>
</soap:Envelope>
----
